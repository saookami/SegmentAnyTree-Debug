üß© SegmentAnyTree Full Fix Log ‚Äî Reproducible & Stable Evaluation Pipeline (CPU-safe)

Author: @HeZhimeng
Context: Fixing the official SegmentAnyTree repo so that evaluation with PointGroup-PAPER runs fully without hydra or multiprocessing deadlocks.

üß± 1. Hydra / YAML loading fix (eval.py)

Problem:
Original eval.py crashes with

omegaconf.errors.ConfigAttributeError: Missing key hydra


because the Hydra config_path wasn‚Äôt properly initialized when using an external .yaml.

Fix:
Explicitly define the config directory and manually load model configs.

# /SegmentAnyTree/eval.py
import os
import hydra
from omegaconf import DictConfig, OmegaConf
from torch_points3d.trainer import Trainer

CONF_PATH = "/mnt/data/project0065/Zhimeng_Data/segmentanytree/SegmentAnyTree/conf"

@hydra.main(config_path=CONF_PATH, config_name="eval", version_base=None)
def main(cfg: DictConfig):
    print("\n[INFO] ‚úÖ Hydra configuration loaded successfully!")
    OmegaConf.set_struct(cfg, False)

    # 1Ô∏è‚É£ Inject model config manually
    model_name = cfg.get("model_name", "PointGroup-PAPER")
    model_cfg_path = os.path.join(CONF_PATH, "models", f"{model_name}.yaml")
    if not os.path.exists(model_cfg_path):
        raise FileNotFoundError(f"‚ùå Model config not found: {model_cfg_path}")

    print(f"[INFO] Loading model config from {model_cfg_path}")
    model_cfg = OmegaConf.load(model_cfg_path)

    if "models" not in cfg:
        cfg.models = OmegaConf.create()
    cfg.models[model_name] = model_cfg
    print(f"[INFO] Injected model config: models.{model_name}")

    # 2Ô∏è‚É£ Merge model config into trainer._model.opt
    trainer = Trainer(cfg)
    if hasattr(trainer, "_model") and model_name in cfg.models:
        from omegaconf import OmegaConf
        print("[INFO] Merging model config into trainer._model.opt ...")
        trainer._model.opt = OmegaConf.merge(trainer._model.opt, cfg.models[model_name])
        print("[INFO] After merge, block_merge_th =", trainer._model.opt.get("block_merge_th", "NOT FOUND"))

    trainer.eval(stage_name="test")

if __name__ == "__main__":
    main()


‚úÖ This ensures that PointGroup-PAPER.yaml is always loaded, even outside Hydra runtime.

‚öôÔ∏è 2. Multiprocessing freeze fix (meanshift_cluster.py)

Problem:
Evaluation freezes at batch 11, caused by multiprocessing.Pool.map() hanging in meanshift_cluster.py.

Fix:
Replace multi-process mapping with single-thread execution (safe and deterministic).

Before:

results = pool.map(partial_meanshift_cluster, all_clusters)
pool.close()
pool.join()


After:

# SAFE SINGLE-THREAD EXECUTION
results = [partial_meanshift_cluster(args) for args in all_clusters]
# pool.close()
# pool.join()


üìçLocation:
/torch_points3d/utils/meanshift_cluster.py (around line 100)

‚úÖ Prevents hanging due to dead child processes, especially on cluster systems or Apptainer containers.

üß† 3. Debug & synchronization prints (PointGroup3heads.py)

Problem:
Hard to locate where the model stalls ‚Äî add fine-grained forward timing.

Fix:
Add _sync() and detailed prints inside forward().

def _sync():
    if torch.cuda.is_available():
        torch.cuda.synchronize()

def forward(self, epoch=-1, **kwargs):
    import time
    t0 = time.time(); _sync(); print("[FWD] start")

    # Backbone
    print("[FWD] backbone in"); _sync()
    backbone_features = self.Backbone(self.input).x
    _sync(); print("[FWD] backbone out, dt=%.3fs" % (time.time()-t0)); t0=time.time()

    # Heads
    print("[FWD] heads in"); _sync()
    semantic_logits = self.Semantic(backbone_features)
    semantic_logits = self.LogSoftLay(semantic_logits)
    offset_logits = self.Offset(backbone_features)
    embed_logits = self.Embed(backbone_features)
    _sync(); print("[FWD] heads out, dt=%.3fs" % (time.time()-t0)); t0=time.time()

    # Clustering
    print("[FWD] proposals/cluster in"); _sync()
    ...
    _sync(); print("[FWD] proposals/cluster out, dt=%.3fs" % (time.time()-t0)); t0=time.time()


‚úÖ Lets you see exactly which stage each batch is at (useful for both CPU and GPU debugging).

üìÑ 4. YAML injection parameters (PointGroup-PAPER.yaml)

File: /conf/models/PointGroup-PAPER.yaml

Ensure it exists and has the following parameters:

block_merge_th: 0.01
block_merge_min_size: 5
block_merge_score_thr: 0.2
block_merge_max_th: 0.2
block_merge_factor: 1.0
block_cluster_th: 0.05
score_thr: 0.09
proposal_score_thr: 0.1
cluster_proposal_score_thr: 0.09
min_cluster_size: 10
instance_npoint_th: 50
max_instance_points: 100000
semantic_label_nclusters: 20


‚úÖ Now the model successfully loads these into self.opt and avoids Missing key block_merge_th errors.

üöÄ 5. Run command (final working version)
apptainer exec --nv \
--bind /mnt/data/project0065/Zhimeng_Data/segmentanytree:/mnt/data/project0065/Zhimeng_Data/segmentanytree \
/mnt/data/project0065/Zhimeng_Data/segmentanytree/sif/segment-any-tree_latest.sif \
python3 /mnt/data/project0065/Zhimeng_Data/segmentanytree/SegmentAnyTree/eval.py \
++training.cuda=0 ++training.num_workers=4 \
++training.checkpoint_dir=/mnt/data/project0065/Zhimeng_Data/segmentanytree/SegmentAnyTree/model_file \
++tracker_options.ply_output=myrun_output_fast.ply \
++pretty_print=true ++logging.progress_refresh_rate=10 \
++logging.wandb_dryrun=true model_name=PointGroup-PAPER


‚úÖ Works end-to-end, no hydra crash, no deadlock, and passes batch 11 successfully.

üß© 6. Optional (GPU acceleration plan)

If you later migrate to GPU:

Use mean_shift_euc_gpu.py (but must ensure all tensors .cpu() before sklearn usage)

Avoid cuml dependency unless CUDA 11.8 + RAPIDS installed

Keep _sync() calls in forward() for timing accuracy

üèÅ Summary Table
File	Change	Purpose
eval.py	manual YAML injection	Fix Hydra config load error
meanshift_cluster.py	disable multiprocessing	Fix hang at batch 11
PointGroup3heads.py	add _sync() prints	Debug runtime & profiling
PointGroup-PAPER.yaml	ensure model params	Fix missing key errors
run command	updated CLI flags	Clean, reproducible call pattern
üí¨ Final Remarks

This setup has been verified to complete PointGroup-PAPER evaluation under Apptainer (CUDA disabled).

Future GPU versions may restore multiprocessing if stability confirmed, but this baseline is scientifically reproducible and safe for large-scale evaluation.
